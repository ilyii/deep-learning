{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit Classification\n",
    "This notebook utilizes deep learning to classify the handwritten digits of the MNIST dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision \n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "datapath = r\"./data\"\n",
    "savepath = r\"./output\"\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  2.2.1+cu121\n",
      "CUDA is available\n",
      "NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "\n",
    "print(\"CUDA is available\" if torch.cuda.is_available() else \"CUDA is not available\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print(torch.cuda.get_device_properties(i).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(savepath, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "val_freq = 1\n",
    "\n",
    "in_dim = 784 #28*28 \n",
    "hidden_dim = 100\n",
    "out_dim = 10\n",
    "\n",
    "mean = 0.1307\n",
    "std = 0.3081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((mean,), (std,))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    MNIST dataset\n",
    "\n",
    "    Args:\n",
    "    - root: path to the csv file\n",
    "    - transform: image transformation\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.data = pd.read_csv(os.path.join(root))\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data.iloc[idx, 1:].values.reshape(28, 28).astype(np.float32)\n",
    "        label = self.data.iloc[idx, 0]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = MNISTDataset(os.path.join(datapath, \"train.csv\"), transform=transform)\n",
    "dataset_test = MNISTDataset(os.path.join(datapath, \"test.csv\"), transform=transform)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size_train, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size_test, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: PyTorch built-in MNIST dataset class \n",
    "\n",
    "# dataset_train =   torchvision.datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "# dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "# dataset_test =  torchvision.datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "# dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display train dataset samples\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "for i in range(n:=9):\n",
    "    idx = random.randint(0, len(dataset_train))\n",
    "    plt.subplot(1,n,i+1)\n",
    "    img = (dataset_train[idx][0].squeeze() * std) + mean\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(f\"{dataset_train[idx][1]}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "  \"\"\"\n",
    "  A simple feedforward neural network with one hidden layer, ReLU and log-softmax.\n",
    "\n",
    "  Args:\n",
    "  - in_dim: input dimension\n",
    "  - hidden_dim: hidden layer dimension\n",
    "  - out_dim: output dimension\n",
    "  \n",
    "  \"\"\"\n",
    "  def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "    super().__init__()\n",
    "    self.layer1 = torch.nn.Linear(in_dim, hidden_dim)\n",
    "    self.layer2 = torch.nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    self.apply(self._init_weights)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.layer1(x))\n",
    "    return F.log_softmax(self.layer2(x), dim=1)\n",
    "  \n",
    "  def _init_weights(self, m):\n",
    "      \"\"\"Normal weight initialization.\"\"\"\n",
    "      if isinstance(m, nn.Linear):\n",
    "          nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "          if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "              nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardNN(\n",
       "  (layer1): Linear(in_features=784, out_features=100, bias=True)\n",
       "  (layer2): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FeedForwardNN(in_dim, hidden_dim, out_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "For training, SGD with momentum is used together with the negative log likelihood loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics tracking\n",
    "\n",
    "train_stats = {\n",
    "    \"step\": [],\n",
    "    \"epoch_loss\": [],\n",
    "    \"iter_loss\": [],\n",
    "}   \n",
    "\n",
    "test_stats = {\n",
    "    \"step\": [],\n",
    "    \"epoch_loss\": [],\n",
    "    \"iter_loss\": [],\n",
    "    \"tp\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch:int, log=False):\n",
    "    \"\"\"\n",
    "    Train one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for idx, data in enumerate(dataloader_train):\n",
    "        img, label = data\n",
    "        img.to(device)\n",
    "        label.to(device)\n",
    "\n",
    "        img = img.view(-1, 28*28)\n",
    "        img, label = img.to(device), label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(img)\n",
    "        loss = criterion(output, label)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if log:\n",
    "            train_stats[\"step\"].append(epoch * len(dataloader_train) + idx)\n",
    "            train_stats[\"iter_loss\"].append(loss.item())\n",
    "\n",
    "\n",
    "    train_loss /= len(dataloader_train)\n",
    "    if log:\n",
    "        train_stats[\"epoch_loss\"].append(train_loss)\n",
    "\n",
    "        if epoch == 0 or train_loss < min(train_stats[\"epoch_loss\"]):\n",
    "            torch.save(model.state_dict(), os.path.join(savepath, \"best.pt\"))\n",
    "    torch.save(model.state_dict(), os.path.join(savepath, \"last.pt\"))\n",
    "    \n",
    "\n",
    "    print(f\"[TRAIN] Epoch: {epoch}/{n_epochs}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "def test(epoch:int, log=False):\n",
    "    \"\"\"\n",
    "    Test at epoch.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for idx, data in enumerate(dataloader_test):\n",
    "            img, label = data\n",
    "            img.to(device)\n",
    "            label.to(device)\n",
    "\n",
    "            img = img.view(-1, 28*28)\n",
    "            img, label = img.to(device), label.to(device)\n",
    "\n",
    "            output = model(img)\n",
    "            loss = criterion(output, label)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            tp = (predicted == label).sum().item()\n",
    "\n",
    "            if log:\n",
    "                test_stats[\"step\"].append(epoch * len(dataloader_test) + idx)\n",
    "                test_stats[\"iter_loss\"].append(loss.item())\n",
    "                test_stats[\"tp\"].append(tp)\n",
    "        \n",
    "        val_loss /= len(dataloader_test)\n",
    "        if log:\n",
    "            test_stats[\"epoch_loss\"].append(val_loss)\n",
    "        print(f\"[VAL] Epoch: {epoch}/{n_epochs}, Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Epoch: 0/10, Loss: 75.5406\n",
      "[VAL] Epoch: 0/10, Loss: 2.3011\n",
      "[TRAIN] Epoch: 1/10, Loss: 2.3012\n",
      "[VAL] Epoch: 1/10, Loss: 2.3011\n",
      "[TRAIN] Epoch: 2/10, Loss: 2.3013\n",
      "[VAL] Epoch: 2/10, Loss: 2.3011\n",
      "[TRAIN] Epoch: 3/10, Loss: 2.3013\n",
      "[VAL] Epoch: 3/10, Loss: 2.3010\n",
      "[TRAIN] Epoch: 4/10, Loss: 2.3013\n",
      "[VAL] Epoch: 4/10, Loss: 2.3010\n",
      "[TRAIN] Epoch: 5/10, Loss: 2.3013\n",
      "[VAL] Epoch: 5/10, Loss: 2.3011\n",
      "[TRAIN] Epoch: 6/10, Loss: 2.3013\n",
      "[VAL] Epoch: 6/10, Loss: 2.3010\n",
      "[TRAIN] Epoch: 7/10, Loss: 2.3013\n",
      "[VAL] Epoch: 7/10, Loss: 2.3010\n",
      "[TRAIN] Epoch: 8/10, Loss: 2.3013\n",
      "[VAL] Epoch: 8/10, Loss: 2.3010\n",
      "[TRAIN] Epoch: 9/10, Loss: 2.3013\n",
      "[VAL] Epoch: 9/10, Loss: 2.3010\n",
      "Finished Training.\n"
     ]
    }
   ],
   "source": [
    "# Training loop on gpu\n",
    "device = \"cuda:0\"\n",
    "model.to(device)\n",
    "total_time = 0\n",
    "for epoch in range(n_epochs):\n",
    "    s_time = time.time()\n",
    "    train(epoch, log=True)\n",
    "    total_time += time.time() - s_time\n",
    "    if epoch % val_freq == 0:\n",
    "        test(epoch, log=True)\n",
    "print(f\"Finished Training in {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Epoch: 0/10, Loss: 2.3013\n",
      "[VAL] Epoch: 0/10, Loss: 2.3010\n",
      "[TRAIN] Epoch: 1/10, Loss: 2.3013\n",
      "[VAL] Epoch: 1/10, Loss: 2.3010\n",
      "[TRAIN] Epoch: 2/10, Loss: 2.3013\n",
      "[VAL] Epoch: 2/10, Loss: 2.3010\n",
      "[TRAIN] Epoch: 3/10, Loss: 2.3013\n",
      "[VAL] Epoch: 3/10, Loss: 2.3010\n",
      "[TRAIN] Epoch: 4/10, Loss: 2.3013\n",
      "[VAL] Epoch: 4/10, Loss: 2.3010\n",
      "[TRAIN] Epoch: 5/10, Loss: 2.3013\n",
      "[VAL] Epoch: 5/10, Loss: 2.3010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mNLLLoss()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m val_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     10\u001b[0m         test(epoch)\n",
      "Cell \u001b[1;32mIn[22], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch, log)\u001b[0m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      6\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ihett\\Workspace\\deep-learning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\ihett\\Workspace\\deep-learning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\ihett\\Workspace\\deep-learning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[8], line 25\u001b[0m, in \u001b[0;36mMNISTDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     22\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 25\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img, label\n",
      "File \u001b[1;32mc:\\Users\\ihett\\Workspace\\deep-learning\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\ihett\\Workspace\\deep-learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ihett\\Workspace\\deep-learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop on cpu\n",
    "device = \"cpu\"\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train(epoch)\n",
    "    if epoch % val_freq == 0:\n",
    "        test(epoch)\n",
    "print(f'Finished Training in {total_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest loss found [2.301154835391909] at epoch 1\n",
      "Accuracy: 1.135\n"
     ]
    }
   ],
   "source": [
    "print(f\"Lowest loss found [{min(train_stats['epoch_loss']):.3f}] at epoch {np.argmin(train_stats['epoch_loss'])}\")\n",
    "print(f'Accuracy: {np.sum(test_stats[\"tp\"])/len(dataset_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "# Epoch loss\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_stats[\"epoch_loss\"], label=\"train\")\n",
    "plt.plot(test_stats[\"epoch_loss\"], label=\"test\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Iteration loss\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_stats[\"iter_loss\"], label=\"train_iter\", alpha=0.5)\n",
    "plt.plot(test_stats[\"iter_loss\"], label=\"test_iter\", alpha=0.5)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
